{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a comprehensive examination of the dataset utilised in the fraud detection model. It details the application of advanced data cleaning, feature extraction, and feature engineering techniques, with the aim of optimising the dataset and identifying the most relevant metrics to enhance predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import plotly.express as px\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bson.json_util import dumps\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from plotnine.data import economics\n",
    "from plotnine import (ggplot, aes, geom_line, geom_bar, labs,theme_minimal, theme, element_rect)\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mongodb is used as the database, as Cassandra is not available. The data is stored in JSON file format, which is subsequently extracted for use in creating the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client  = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"local\"]\n",
    "collections = db[\"inueron.ai data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [dumps(data) for data in collections.find()]\n",
    "df = pd.read_json(f\"[{','.join(records)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TX_DATETIME'] = df['TX_DATETIME'].apply(lambda x: datetime.fromisoformat(x['$date'].replace('Z', '+00:00')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TX_FRAUD'] = df['TX_FRAUD'].apply(lambda x: 0 if x == 'Legitimate' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'TRANSACTION_ID', 'TX_DATETIME', 'CUSTOMER_ID', 'TERMINAL_ID',\n",
       "       'TX_AMOUNT', 'TX_TIME_SECONDS', 'TX_TIME_DAYS', 'TX_FRAUD_SCENARIO',\n",
       "       'TX_FRAUD', 'TX_YEAR'],\n",
       "      dtype='object')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if df.columns.dtype == \"object\":\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TX_MONTH'] = pd.DatetimeIndex(df['TX_DATETIME']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TX_YEAR'] = pd.DatetimeIndex(df['TX_DATETIME']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['_id'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_night(timestamp): \n",
    "        tx_hour = timestamp.hour \n",
    "        is_night = tx_hour <= 6 \n",
    "        return int(is_night)\n",
    "def is_weekend(timestamp):\n",
    "        tx_weekend = timestamp.weekday()\n",
    "        is_weekend = tx_weekend >= 5\n",
    "        return int(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TX_NIGHT'] = df['TX_DATETIME'].apply(is_night)\n",
    "df['TX_WEEKEND'] = df['TX_DATETIME'].apply(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TX_IS_AMOUNT_HIGH'] = df['TX_AMOUNT'].apply(lambda x: 1 if x >= 150 else 0)\n",
    "df['TX_TIME_TAKEN_HIGH'] = df['TX_TIME_SECONDS'].apply(lambda x: 1 if x > 7903233.708571933 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_scenarios = pd.get_dummies(df['TX_FRAUD_SCENARIO'], prefix=\"FRAUD_SCENARIO\", dtype = int)\n",
    "df = pd.concat([df, fraud_scenarios], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         TRANSACTION_ID  ... FRAUD_SCENARIO_Random Fraud\n",
       "0                316917  ...                           0\n",
       "1                316918  ...                           0\n",
       "2                316919  ...                           0\n",
       "3                316920  ...                           0\n",
       "4                316921  ...                           0\n",
       "...                 ...  ...                         ...\n",
       "1754150          374287  ...                           0\n",
       "1754151          374288  ...                           0\n",
       "1754152          374289  ...                           0\n",
       "1754153          374290  ...                           0\n",
       "1754154          374291  ...                           0\n",
       "\n",
       "[1754155 rows x 19 columns]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA has been done below for visualisation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Transaction Amount Distribution\",\n",
    "        \"Hourly Transaction Patterns\",\n",
    "        \"Frauds Reported by Scenario\",\n",
    "        \"Monthly Transaction Trends\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=df['TX_AMOUNT'], name=\"Transaction Amount\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "hourly_data = df.groupby('TX_HOUR').size().reset_index(name='Count')\n",
    "fig.add_trace(\n",
    "    go.Bar(x=hourly_data['TX_HOUR'], y=hourly_data['Count'], name=\"Hourly Transactions\"),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fraud_scenario = df.groupby('TX_FRAUD_SCENARIO')['TX_FRAUD'].apply(lambda x: (x == 1).sum())\n",
    "fig.add_trace(\n",
    "    go.Bar(x=fraud_scenario.index, y=fraud_scenario.values, name=\"Frauds by Scenario\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "monthly_data = df.groupby('TX_MONTH').size().reset_index(name='Count')\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=monthly_data['TX_MONTH'], y=monthly_data['Count'], mode='lines+markers', name=\"Monthly Trends\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Comprehensive EDA Dashboard\",\n",
    "    height=800,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12039\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean = df['TX_AMOUNT'].mean()\n",
    "std = df['TX_AMOUNT'].std()\n",
    "threshold = 3\n",
    "outliers = []\n",
    "twentyfithpercentile = np.percentile(df['TX_AMOUNT'], 25)\n",
    "seventyfifthpercentile = np.percentile(df['TX_AMOUNT'], 75)\n",
    "outlier_count = 0\n",
    "for index,data in df['TX_AMOUNT'].items():\n",
    "    z_score = (data - mean) / std\n",
    "    if z_score > threshold:\n",
    "        df.at[index, 'TX_AMOUNT'] = seventyfifthpercentile\n",
    "        outlier_count += 1\n",
    "    elif z_score < -threshold:\n",
    "        df.at[index, 'TX_AMOUNT'] = twentyfithpercentile\n",
    "        outlier_count += 1\n",
    "\n",
    "\n",
    "print(outlier_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TX_HOUR'] = df['TX_DATETIME'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TX_RUSH_HOUR'] = df['TX_HOUR'].apply(lambda x: 1 if x in [8,9,10,16,17,18] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = ['TX_FRAUD', 'TX_DATETIME', 'TX_FRAUD_SCENARIO', 'FRAUD_SCENARIO_Large Amount', 'FRAUD_SCENARIO_Leaked data', 'FRAUD_SCENARIO_Legitimate', 'FRAUD_SCENARIO_Random Fraud'])\n",
    "y = df['TX_FRAUD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TX_AMOUNT_ROUNDED'] = df['TX_AMOUNT'].apply(lambda x: 1 if x % 100 == 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TX_LUNCH_TIME'] = df['TX_HOUR'].apply(lambda x: 1 if x in [11,12,13,14] else 0)\n",
    "df['TX_LATE_NIGHT'] = df['TX_HOUR'].apply(lambda x: 1 if x in [23,0,1,2,3] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_smo, y_train_smo = smote.fit_resample(x_train, y_train)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaler = scaler.fit_transform(X_train_smo)\n",
    "X_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various machine learning models, including Decision Tree, CatBoost, Gradient Boosting, and Random Forest, were evaluated. However, XGBoost, being an ensemble method that leverages weak learners, demonstrated superior performance in achieving an optimal balance between precision and recall. As a result, XGBoost was selected for the final model. Given the highly imbalanced nature of the dataset, XGBoost performed consistently well. Hyperparameter tuning was conducted using RandomizedSearchCV, as the computational resources for GridSearchCV were unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
       "\n",
       "Classification Report:\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0     0.9931    0.7467    0.8525    521843\n",
       "           1     0.0128    0.3892    0.0248      4404\n",
       "\n",
       "    accuracy                         0.7437    526247\n",
       "   macro avg     0.5030    0.5679    0.4386    526247\n",
       "weighted avg     0.9849    0.7437    0.8455    526247\n",
       "\n",
       "\n",
       "Best Parameters:\n",
       "subsample: 0.8\n",
       "scale_pos_weight: 3\n",
       "n_estimators: 500\n",
       "min_child_weight: 5\n",
       "max_depth: 10\n",
       "learning_rate: 0.2\n",
       "gamma: 0.2\n",
       "colsample_bytree: 0.8\n",
       "\n",
       "Best Cross-Validation Score: 0.8834\n",
       "\n",
       "Top 5 Parameter Combinations:\n",
       "                                                                                                                                                                 params  mean_test_score  std_test_score\n",
       "7   {'subsample': 0.8, 'scale_pos_weight': 3, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 10, 'learning_rate': 0.2, 'gamma': 0.2, 'colsample_bytree': 0.8}         0.883408        0.000359\n",
       "17    {'subsample': 0.6, 'scale_pos_weight': 2, 'n_estimators': 400, 'min_child_weight': 3, 'max_depth': 10, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 1.0}         0.859604        0.001211\n",
       "47   {'subsample': 0.7, 'scale_pos_weight': 3, 'n_estimators': 300, 'min_child_weight': 7, 'max_depth': 9, 'learning_rate': 0.2, 'gamma': 0.1, 'colsample_bytree': 0.6}         0.827802        0.000706\n",
       "46     {'subsample': 0.8, 'scale_pos_weight': 2, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 9, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 1.0}         0.822070        0.001646\n",
       "16   {'subsample': 0.6, 'scale_pos_weight': 3, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 9, 'learning_rate': 0.1, 'gamma': 0.2, 'colsample_bytree': 0.6}         0.820542        0.001035\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, make_scorer, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'scale_pos_weight': [1, 2, 3, 5],\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "base_model = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    objective='binary:logistic',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_scaler, y_train_smo)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Cross-Validation Score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "cv_results = cv_results.sort_values('rank_test_score')\n",
    "\n",
    "print(\"\\nTop 5 Parameter Combinations:\")\n",
    "top_params = cv_results[['params', 'mean_test_score', 'std_test_score']].head()\n",
    "print(top_params.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the model performance below to get a great understanding.SHAP importance feature provides additional details on how each performance has contributed to the overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "explainer = shap.Explainer(best_model)\n",
    "shap_values = explainer(x_test)\n",
    "shap_values = explainer.shap_values(x_test[:1000])\n",
    "\n",
    "shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': x_test.columns,\n",
    "    'importance': shap_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\\n\", feature_importance)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        \"AUC-ROC Curve\",\n",
    "        \"Confusion Matrix\",\n",
    "        \"SHAP Feature Importance\",\n",
    "        \"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC Curve', line=dict(color='blue')),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random', line=dict(dash='dash', color='red')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=cm, colorscale='Viridis', showscale=True, text=cm, hovertemplate=\"%{text}\", colorbar=dict(title=\"Count\")),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=feature_importance['feature'], y=feature_importance['importance'], name='Feature Importance', marker=dict(color='green')),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Model Evaluation and Feature Importance\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
